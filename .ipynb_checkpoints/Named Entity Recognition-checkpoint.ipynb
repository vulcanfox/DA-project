{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc617738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"tner/bionlp2004\", \n",
    "    cache_dir='./data_cache'\n",
    ")\n",
    "\n",
    "print(f'The dataset is a dictionary with {len(dataset)} splits: \\n\\n{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efe049",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_ner = [item['tokens'] for item in dataset['train']]\n",
    "train_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['train']]\n",
    "\n",
    "val_sentences_ner = [item['tokens'] for item in dataset['validation']]\n",
    "val_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['validation']]\n",
    "\n",
    "test_sentences_ner = [item['tokens'] for item in dataset['test']]\n",
    "test_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbc3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training sentences = {len(train_sentences_ner)}')\n",
    "print(f'Number of validation sentences = {len(val_sentences_ner)}')\n",
    "print(f'Number of test sentences = {len(test_sentences_ner)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e033546",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'What does one instance look like from the training set? \\n\\n{train_sentences_ner[234]}')\n",
    "print(f'...and here is its corresponding label \\n\\n{train_labels_ner[234]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5897c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of unique labels: {np.unique(np.concatenate(train_labels_ner))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa523db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from labels to the tags\n",
    "\n",
    "id2label = {\n",
    "    \"O\": 0,\n",
    "    \"B-DNA\": 1,\n",
    "    \"I-DNA\": 2,\n",
    "    \"B-protein\": 3,\n",
    "    \"I-protein\": 4,\n",
    "    \"B-cell_type\": 5,\n",
    "    \"I-cell_type\": 6,\n",
    "    \"B-cell_line\": 7,\n",
    "    \"I-cell_line\": 8,\n",
    "    \"B-RNA\": 9,\n",
    "    \"I-RNA\": 10\n",
    "}\n",
    "\n",
    "mapping = {v:k for k, v in id2label.items()}\n",
    "print(mapping[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [list(zip(train_sentences_ner[index],[mapping[int(i)]for i in train_labels_ner[index]]))for index, sentence in enumerate(train_sentences_ner)]\n",
    "test_set = [list(zip(test_sentences_ner[index],[mapping[int(i)]for i in test_labels_ner[index]]))for index, sentence in enumerate(test_sentences_ner)]\n",
    "test_tokens = [tok for tok in test_sentences_ner]\n",
    "test_tags = [[mapping[int(i)]for i in item] for item in test_labels_ner]\n",
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "\n",
    "tagger = CRFTagger(verbose= True)\n",
    "tagger.train(train_set,'model.crf.my_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)\n",
    "print(predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "269a76b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class protein = 0.6906707201216036\n",
      "F1 score for class cell_type = 0.7072816942268639\n",
      "F1 score for class DNA = 0.6420369445831253\n",
      "F1 score for class cell_line = 0.5381526104417671\n",
      "F1 score for class RNA = 0.6233766233766234\n",
      "Macro-average f1 score = 0.6403037185499967\n"
     ]
    }
   ],
   "source": [
    "def extract_spans(tagged_sents):\n",
    "    \"\"\"\n",
    "    Extract a list of tagged spans for each named entity type, \n",
    "    where each span is represented by a tuple containing the \n",
    "    start token and end token indexes.\n",
    "    \n",
    "    returns: a dictionary containing a list of spans for each entity type.\n",
    "    \"\"\"\n",
    "    spans = {}\n",
    "        \n",
    "    for sidx, sent in enumerate(tagged_sents):\n",
    "        start = -1\n",
    "        entity_type = None\n",
    "        for i, (tok, lab) in enumerate(sent):\n",
    "            if 'B-' in lab:\n",
    "                start = i\n",
    "                end = i + 1\n",
    "                entity_type = lab[2:]\n",
    "            elif 'I-' in lab:\n",
    "                end = i + 1\n",
    "            elif lab == 'O' and start >= 0:\n",
    "                \n",
    "                if entity_type not in spans:\n",
    "                    spans[entity_type] = []\n",
    "                \n",
    "                spans[entity_type].append((start, end, sidx))\n",
    "                start = -1      \n",
    "        # Sometimes an I-token is the last token in the sentence, so we still have to add the span to the list\n",
    "        if start >= 0:    \n",
    "            if entity_type not in spans:\n",
    "                spans[entity_type] = []\n",
    "                \n",
    "            spans[entity_type].append((start, end, sidx))\n",
    "                \n",
    "    return spans\n",
    "\n",
    "\n",
    "def cal_span_level_f1(test_sents, test_sents_with_pred):\n",
    "    # get a list of spans from the test set labels\n",
    "    true_spans = extract_spans(test_sents)\n",
    "\n",
    "    # get a list of spans predicted by our tagger\n",
    "    predicted_spans = extract_spans(test_sents_with_pred)\n",
    "    \n",
    "    # compute the metrics for each class:\n",
    "    F1_score_for_each_class = []\n",
    "    \n",
    "    named_entity_types = true_spans.keys()  # get the list of named entity types (not the tags)\n",
    "    \n",
    "    score_printer(named_entity_types, true_spans, predicted_spans, F1_score_for_each_class)\n",
    "    \n",
    "def score_printer(named_entity_types, true_spans, predicted_spans, F1_score_for_each_class):\n",
    "    \n",
    "    # Manually calculating F1, precision, recall. \n",
    "    #\n",
    "    for named_entity_type in named_entity_types:\n",
    "        # We loop through all the named entity tpes\n",
    "        # set TP, FN, and FP to zero.\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        false_negative = 0\n",
    "        \n",
    "        for span in true_spans[named_entity_type]:\n",
    "            # check if current true span not in the predicted spans\n",
    "            if span not in predicted_spans[named_entity_type]:\n",
    "                # If so...increment false negative value.\n",
    "                false_negative += 1\n",
    "        \n",
    "        for span in predicted_spans[named_entity_type]:\n",
    "            # check if current predicted span in the true spans\n",
    "            if span in true_spans[named_entity_type]:\n",
    "                # If so, increment true positive val\n",
    "                true_positive = true_positive + 1\n",
    "            else:\n",
    "                # otherwise increment false negative val\n",
    "                false_positive = false_positive + 1       \n",
    "        \n",
    "            \n",
    "        if true_positive + false_negative== 0:\n",
    "            # set recall\n",
    "            recall = 0\n",
    "        else:\n",
    "            # calculate recall using TP and FN\n",
    "            recall = true_positive / float(true_positive + false_negative)\n",
    "\n",
    "            \n",
    "        if true_positive + false_positive == 0:\n",
    "            # Set precision\n",
    "            precision = 0\n",
    "        else:\n",
    "            # calculate precision using FP and TP\n",
    "            precision = true_positive / float(false_positive + true_positive)\n",
    "            \n",
    "\n",
    "        if recall + precision == 0:\n",
    "            # Set F1 score\n",
    "            F1 = 0\n",
    "        else:\n",
    "            # Calculate F1 using precision and recall\n",
    "            F1 = 2 * precision * recall / (precision + recall)\n",
    "            \n",
    "\n",
    "        F1_score_for_each_class.append(F1)\n",
    "        print(f'F1 score for class {named_entity_type} = {F1}')\n",
    "        \n",
    "    print(f'Macro-average f1 score = {np.mean(F1_score_for_each_class)}')\n",
    "\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d3708cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "\n",
    "class CustomCRFTagger(CRFTagger):\n",
    "    _current_tokens = None\n",
    "    \n",
    "    def _get_features(self, tokens, idx):\n",
    "            \"\"\"\n",
    "            Extract basic features about this word including\n",
    "                - Current word\n",
    "                - is it capitalized?\n",
    "                - Does it have punctuation?\n",
    "                - Does it have a number?\n",
    "                - Suffixes up to length 3\n",
    "\n",
    "            Note that : we might include feature over previous word, next word etc.\n",
    "\n",
    "            :return: a list which contains the features\n",
    "            :rtype: list(str)\n",
    "            \"\"\"\n",
    "            token = tokens[idx]\n",
    "\n",
    "            feature_list = []\n",
    "\n",
    "            if not token:\n",
    "                return feature_list\n",
    "\n",
    "            # Capitalization\n",
    "            if token[0].isupper():\n",
    "                feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "            # Number\n",
    "            if re.search(self._pattern, token) is not None:\n",
    "                feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "            # Punctuation\n",
    "            punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "            if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "                feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "            # Suffix up to length 3\n",
    "            if len(token) > 1:\n",
    "                feature_list.append(\"SUF_\" + token[-1:])\n",
    "            if len(token) > 2:\n",
    "                feature_list.append(\"SUF_\" + token[-2:])\n",
    "            if len(token) > 3:\n",
    "                feature_list.append(\"SUF_\" + token[-3:])\n",
    "\n",
    "                \n",
    "            # Current word\n",
    "            feature_list.append(\"WORD_\" + token)\n",
    "            \n",
    "            ### WRITE YOUR OWN CODE HERE ###\n",
    "            if idx > 0:\n",
    "                feature_list.append(\"PREVWORD_\" + tokens[idx-1])\n",
    "            if idx < len(tokens)-1:\n",
    "                feature_list.append(\"NEXTWORD_\" + tokens[idx+1])\n",
    "                \n",
    "            ####\n",
    "\n",
    "            return feature_list\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CRF NER tagger\n",
    "def train_CustomCRF_NER_tagger(train_set):\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tagger = CustomCRFTagger()\n",
    "    tagger.train(train_set, 'model.crf.cust_tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CustomCRF_NER_tagger(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b714cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Improve the CRF NER tagger using parts of speech (see lab 5) as additional features.\n",
    "from nltk.tag import pos_tag\n",
    "import nltk as nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "class CRFTaggerWithPOS(CustomCRFTagger):\n",
    "    _current_tokens = None\n",
    "    \n",
    "    def _get_features(self, tokens, index):\n",
    "        \"\"\"\n",
    "        Extract the features for a token and append the POS tag as an additional feature.\n",
    "        \"\"\"\n",
    "        basic_features = super()._get_features(tokens, index)\n",
    "        \n",
    "        # Get the pos tags for the current sentence and save it\n",
    "        if tokens != self._current_tokens:\n",
    "            self._pos_tagged_tokens = pos_tag(tokens)\n",
    "            self._current_tokens = tokens\n",
    "            \n",
    "            \n",
    "        ### WRITE YOUR OWN CODE HERE\n",
    "        basic_features.append(self._pos_tagged_tokens[index][1])\n",
    "        ###\n",
    "        \n",
    "        return basic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CRF NER tagger\n",
    "def train_CRF_NER_tagger_with_POS(train_set):\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tagger = CRFTaggerWithPOS()\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CRF_NER_tagger_with_POS(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13aa35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
