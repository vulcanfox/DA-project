{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f426e189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc2779f17f24b59a70c15a4883163fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset bio_nlp2004/bionlp2004 to ./data_cache\\tner___bio_nlp2004\\bionlp2004\\1.0.0\\9f41d3f0270b773c2762dee333ae36c29331e2216114a57081f77639fdb5e904...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def83ba580414da7ba305bca2e92a2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656155b7c3364252805f97ce29457544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ecf61c1420488690fd22df743d5671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7ba2f6d39a44089ee2b5211ae7e6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/660k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350b128c693849be853e61300e8d7b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset bio_nlp2004 downloaded and prepared to ./data_cache\\tner___bio_nlp2004\\bionlp2004\\1.0.0\\9f41d3f0270b773c2762dee333ae36c29331e2216114a57081f77639fdb5e904. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a4ba07d10148b6916cbc87b3389b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is a dictionary with 3 splits: \n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 16619\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 1927\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 3856\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"tner/bionlp2004\", \n",
    "    cache_dir='./data_cache'\n",
    ")\n",
    "\n",
    "print(f'The dataset is a dictionary with {len(dataset)} splits: \\n\\n{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda2d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_ner = [item['tokens'] for item in dataset['train']]\n",
    "train_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['train']]\n",
    "\n",
    "val_sentences_ner = [item['tokens'] for item in dataset['validation']]\n",
    "val_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['validation']]\n",
    "\n",
    "test_sentences_ner = [item['tokens'] for item in dataset['test']]\n",
    "test_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f72eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences = 16619\n",
      "Number of validation sentences = 1927\n",
      "Number of test sentences = 3856\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training sentences = {len(train_sentences_ner)}')\n",
    "print(f'Number of validation sentences = {len(val_sentences_ner)}')\n",
    "print(f'Number of test sentences = {len(test_sentences_ner)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f53e838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does one instance look like from the training set? \n",
      "\n",
      "['Hence', ',', 'PPAR', 'can', 'positively', 'or', 'negatively', 'influence', 'TH', 'action', 'depending', 'on', 'TRE', 'structure', 'and', 'THR', 'isotype', '.']\n",
      "...and here is its corresponding label \n",
      "\n",
      "['0', '0', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '3', '4', '0']\n"
     ]
    }
   ],
   "source": [
    "print(f'What does one instance look like from the training set? \\n\\n{train_sentences_ner[234]}')\n",
    "print(f'...and here is its corresponding label \\n\\n{train_labels_ner[234]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b36042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: ['0' '1' '10' '2' '3' '4' '5' '6' '7' '8' '9']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of unique labels: {np.unique(np.concatenate(train_labels_ner))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c67a35c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-DNA', 2: 'I-DNA', 3: 'B-protein', 4: 'I-protein', 5: 'B-cell_type', 6: 'I-cell_type', 7: 'B-cell_line', 8: 'I-cell_line', 9: 'B-RNA', 10: 'I-RNA'}\n"
     ]
    }
   ],
   "source": [
    "# mapping from labels to the tags\n",
    "\n",
    "id2label = {\n",
    "    \"O\": 0,\n",
    "    \"B-DNA\": 1,\n",
    "    \"I-DNA\": 2,\n",
    "    \"B-protein\": 3,\n",
    "    \"I-protein\": 4,\n",
    "    \"B-cell_type\": 5,\n",
    "    \"I-cell_type\": 6,\n",
    "    \"B-cell_line\": 7,\n",
    "    \"I-cell_line\": 8,\n",
    "    \"B-RNA\": 9,\n",
    "    \"I-RNA\": 10\n",
    "}\n",
    "\n",
    "mapping = {v:k for k, v in id2label.items()}\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b867bf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 'Hence'),\n",
       " ('0', ','),\n",
       " ('3', 'PPAR'),\n",
       " ('0', 'can'),\n",
       " ('0', 'positively'),\n",
       " ('0', 'or'),\n",
       " ('0', 'negatively'),\n",
       " ('0', 'influence'),\n",
       " ('0', 'TH'),\n",
       " ('0', 'action'),\n",
       " ('0', 'depending'),\n",
       " ('0', 'on'),\n",
       " ('1', 'TRE'),\n",
       " ('0', 'structure'),\n",
       " ('0', 'and'),\n",
       " ('3', 'THR'),\n",
       " ('4', 'isotype'),\n",
       " ('0', '.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = [list(zip(train_labels_ner[index],train_sentences_ner[index]))for index, sentence in enumerate(train_sentences_ner)]\n",
    "#train_set = [[token for token in s] for s in train_sentences_ner][:-1]\n",
    "train_set[234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d9346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
