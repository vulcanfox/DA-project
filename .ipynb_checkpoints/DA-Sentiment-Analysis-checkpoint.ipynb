{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdef17b",
   "metadata": {},
   "source": [
    "# Data analytics coursework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c81566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the financial_phrasebank dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3db0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [\n",
    "    'data/FiQA_ABSA_task1/task1_headline_ABSA_train.json',\n",
    "    'data/FiQA_ABSA_task1/task1_post_ABSA_train.json'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29bb0c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 1111\n",
      "Number of labels: 1111\n",
      "Number of negative labels: 310\n",
      "Number of neutral labels: 195\n",
      "Number of positive labels: 606\n"
     ]
    }
   ],
   "source": [
    "## Loading data from JSON\n",
    "import json\n",
    "\n",
    "def load_fiqa_sa_from_json(json_files):\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "\n",
    "    for file in json_files:\n",
    "        with open(file, 'r', encoding='UTF-8') as handle:\n",
    "            dataf = json.load(handle)\n",
    "\n",
    "        dataf_text = [dataf[k][\"sentence\"] for k in dataf.keys()]\n",
    "        # print(len(dataf_text))\n",
    "        train_text.extend(dataf_text)\n",
    "\n",
    "        dataf_labels = [float(dataf[k][\"info\"][0][\"sentiment_score\"]) for k in dataf.keys()]\n",
    "        # print(len(dataf_labels))\n",
    "        train_labels.extend(dataf_labels)\n",
    "\n",
    "    train_text = np.array(train_text)\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "    return train_text, train_labels\n",
    "\n",
    "\n",
    "def threshold_scores(scores):\n",
    "    \"\"\"\n",
    "    Convert sentiment scores to discrete labels.\n",
    "    0 = negative.\n",
    "    1 = neutral.\n",
    "    2 = positive.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for score in scores:\n",
    "        if score < -0.2:\n",
    "            labels.append(0)\n",
    "        elif score > 0.2:\n",
    "            labels.append(2)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "            \n",
    "    return np.array(labels)\n",
    "\n",
    "\n",
    "all_text, all_labels = load_fiqa_sa_from_json(train_files)\n",
    "    \n",
    "print(f'Number of instances: {len(all_text)}')\n",
    "print(f'Number of labels: {len(all_labels)}')\n",
    "\n",
    "all_labels = threshold_scores(all_labels)\n",
    "print(f'Number of negative labels: {np.sum(all_labels==0)}')\n",
    "print(f'Number of neutral labels: {np.sum(all_labels==1)}')\n",
    "print(f'Number of positive labels: {np.sum(all_labels==2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5f008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances = 754\n",
      "Number of validation instances = 134\n",
      "Number of test instances = 223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split test data from training data\n",
    "train_documents, test_documents, train_labels, test_labels = train_test_split(\n",
    "    all_text, \n",
    "    all_labels, \n",
    "    test_size=0.2, \n",
    "    stratify=all_labels  # make sure the same proportion of labels is in the test set and training set\n",
    ")\n",
    "\n",
    "# Split validation data from training data\n",
    "train_documents, val_documents, train_labels, val_labels = train_test_split(\n",
    "    train_documents, \n",
    "    train_labels, \n",
    "    test_size=0.15, \n",
    "    stratify=train_labels  # make sure the same proportion of labels is in the test set and training set\n",
    ")\n",
    "\n",
    "print(f'Number of training instances = {len(train_documents)}')\n",
    "print(f'Number of validation instances = {len(val_documents)}')\n",
    "print(f'Number of test instances = {len(test_documents)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f759bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does one instance look like from the training set? \n",
      "\n",
      "UPDATE 2-Pricey beers lift SABMiller's quarterly underlying sales\n",
      "...and here is its corresponding label \n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(f'What does one instance look like from the training set? \\n\\n{train_documents[234]}')\n",
    "print(f'...and here is its corresponding label \\n\\n{train_labels[234]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443a6c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adnan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adnan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization and N-Gramming\n",
    "import nltk as nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7aa7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, tweets):\n",
    "        return [self.wnl.lemmatize(self.wnl.lemmatize(self.wnl.lemmatize(tok, pos='n'), pos='v'), pos='a') for tok in word_tokenize(tweets)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eff5b79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that wa', 'wa quick', 'quick ,', ', out', 'out $', '$ fio', 'fio 21.35', '21.35 ,', ', market', 'market tank', 'tank ,', ', scale', 'scale down', 'down long', 'long now', '$ cien', 'cien http', 'http :', ': //stks.co/dzu', '//stks.co/dzu cien']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), ngram_range=(2,2))\n",
    "vectorizer.fit(train_documents)\n",
    "X_train = vectorizer.transform(train_documents)\n",
    "X_test = vectorizer.transform(test_documents)\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "978adde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8330\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e4fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
